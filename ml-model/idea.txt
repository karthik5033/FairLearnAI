To build the AI Chatbot with Prompt Filter Safety, you NEED a backend layer (even if your app is Next.js) because you must NOT let the frontend directly hit the AI model.
If students can call the LLM directly â†’ they can bypass your rules â†’ game over.

So the architecture is:

âœ… Frontend Chat UI â†’ âœ… Backend AI Gateway â†’ âœ… Prompt Filter Model â†’ âœ… Policy + Rewrite â†’ âœ… LLM â†’ âœ… Response back

âœ… What backend do we need? (Simple and clean)

Since you're already using Next.js, the best approach is:

âœ… Use Next.js API Routes (built-in backend)

You donâ€™t need a separate backend server in the beginning.

Create endpoints like:

POST /api/ai/classify

POST /api/ai/chat

This is enough for a prototype + demo.

ğŸ§  The Prompt Filter Model (what it should do)

This model recognizes intent and decides if the student is trying to cheat.

Labels:

ALLOWED

HINT_ONLY

DISALLOWED

OFF_TOPIC

Example classification:

Student Prompt	Label
â€œExplain BFS and DFS differenceâ€	ALLOWED
â€œSolve this entire integration questionâ€	HINT_ONLY
â€œGive final answers onlyâ€	DISALLOWED
â€œTell me a jokeâ€	OFF_TOPIC
ğŸ”¥ Best Practical Approach (highest accuracy + easiest build)

You have 2 ways:

âœ… Option 1 (BEST for now): Hybrid filter = Model + Rules

This gives high accuracy without training a huge model.

Step 1: Rule-based fast check (instant)

Detect obvious cheating phrases:

"final answer"

"give me the solution"

"solve completely"

"answer key"

"entire paper"

"only answers no explanation"

If found â†’ directly DISALLOWED âœ…

Step 2: If not obvious â†’ run small ML classifier

TF-IDF + Logistic Regression

outputs label + confidence

Step 3: If confidence is low â†’ fallback to LLM classifier

(only for edge cases)

This hybrid setup is ğŸ”¥ very accurate.

ğŸ—ï¸ Backend Flow (exact logic)

When student sends message:

âœ… Step-by-step pipeline

Receive prompt from frontend

Run classifier â†’ get label

Apply teacher policy rules (exam mode etc.)

If label = HINT_ONLY â†’ rewrite prompt

Send safe prompt to LLM

Guard output so it doesnâ€™t leak final answers

Return response + label to frontend

ğŸ” Safety Rules (must-have)

Even if the prompt is safe, the LLM might output a full answer.
So we add response guard.

Example â€œHint Modeâ€ Safe System Prompt

You are a tutor. Provide hints and steps only. Do not give final answer.

This is 10x safer than just rewriting prompts.

âœ… The 2 API Routes you need (minimum)
1ï¸âƒ£ /api/ai/classify

Purpose: just classification (optional)
Input:

{ "prompt": "solve this full question" }


Output:

{ "label": "HINT_ONLY", "confidence": 0.81 }

2ï¸âƒ£ /api/ai/chat

Purpose: full chat pipeline
Input:

{
  "prompt": "solve this derivative question",
  "mode": "practice",
  "userRole": "student"
}


Output:

{
  "label": "HINT_ONLY",
  "answer": "Start by identifying the rule..."
}

ğŸ’¾ Where to store data? (local-first)

For prototype:
âœ… Store chat history in browser (localStorage/IndexedDB)

Backend only stores:

blocked prompt count (optional)

analytics (optional)

No external DB required.

âœ… What you should build NOW (sequence)

This is the correct order:

âœ… Step A: Create /api/ai/chat

Hardcode:

exam mode = false

filter rules basic

âœ… Step B: Add prompt classification (rule-based)

Use keywords â†’ DISALLOWED / HINT_ONLY

âœ… Step C: Add ML model later

TF-IDF + Logistic Regression trained model

Start with rules first because it works instantly.